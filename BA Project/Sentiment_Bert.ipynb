{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import wget\n",
    "from zipfile import ZipFile\n",
    "import string\n",
    "import preprocessor.api as p\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexs = {'atheism': ('Lexicon_Atheism', 1),\n",
    "        'climate': ('Lexicon_Climate', 2),\n",
    "       'feminism': ('Lexicon_Feminism',3),\n",
    "       'hillary': ('Lexicon_Hillary', 4),\n",
    "       'abortion': ('Lexicon_Abortion', 5)}\n",
    "\n",
    "target_word = 'atheism'\n",
    "lexicon_col = lexs.get(target_word)[0]\n",
    "target_col = lexs.get(target_word)[1]\n",
    "filename1 = 'raw_train_'+target_word+'.csv'\n",
    "filename2 = 'raw_val_'+target_word+'.csv'\n",
    "filename3 = 'raw_test_'+target_word+'.csv'\n",
    "filename_lexicon = 'stance_lexicon_emnlp.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa5ad5234f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = 'http://alt.qcri.org/semeval2016/task6/data/uploads/stancedataset.zip'\n",
    "#filename = wget.download(url)\n",
    "#with ZipFile(filename, 'r') as zipObj:\n",
    "#    zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data (path_train, path_test):\n",
    "    #train data\n",
    "    dt = pd.read_csv(path_train, engine='python' )\n",
    "    raw_train, raw_val = train_test_split(dt, test_size=0.15, random_state=1)    \n",
    "    raw_train.to_csv('raw_train.csv', index=False) #saving all train data\n",
    "    raw_val.to_csv('raw_val.csv', index=False) #saving 15% val data\n",
    "    #test data\n",
    "    dt2 = pd.read_csv(path_test, engine='python' )\n",
    "    raw_test = dt2[dt2['Target']!= 'Donald Trump'] #excluding trump target\n",
    "    raw_test.to_csv('raw_test.csv', index=False) #saving all test data\n",
    "    return raw_train, raw_val, raw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'StanceDataset/train.csv'\n",
    "path_test = 'StanceDataset/test.csv'\n",
    "\n",
    "raw_train, raw_val, raw_test = upload_data (path_train, path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = {'Legalization of Abortion':'abortion',\n",
    "     'Feminist Movement': 'feminism',\n",
    "     'Hillary Clinton': 'hillary',\n",
    "     'Climate Change is a Real Concern': 'climate',\n",
    "     'Atheism': 'atheism'     \n",
    "    }\n",
    "\n",
    "#train, validation and test datasets, filtered by targets, overall 3 datasets * 5 targets = 15 files\n",
    "for key in tar.keys():\n",
    "        filt_tr = raw_train[raw_train['Target']=='{}'.format(key)]           \n",
    "        filt_tr.to_csv('raw_train' +'_'+ tar.get('{}'.format(key))+'.csv',index=False)\n",
    "        \n",
    "        filt_val = raw_val[raw_val['Target']=='{}'.format(key)]           \n",
    "        filt_val.to_csv('raw_val' +'_'+ tar.get('{}'.format(key))+'.csv',index=False)\n",
    "        \n",
    "        filt_tst = raw_test[raw_test['Target']=='{}'.format(key)]           \n",
    "        filt_tst.to_csv('raw_test' +'_'+ tar.get('{}'.format(key))+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "def Data_Clean(strings)::x\n",
    "    p.set_options('urls','emojis','reserved_words')\n",
    "    clean_data = p.clean(strings) # using lib to clean URL,hashtags...\n",
    "    clean_data = re.findall(r\"[A-Za-z]+|[,.!?&/<>=$]\",clean_data)\n",
    "#    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "#    clean_data = [re_punc.sub('', w) for w in clean_data]\n",
    "#    clean_data = [x.lower() for x in clean_data]\n",
    "\n",
    "    return \" \".join(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "def Load_Transform_Data(filename):\n",
    "    #Loading data\n",
    "    filename = [filename]\n",
    "    concat_text = pd.DataFrame()\n",
    "    raw_text = pd.read_csv(filename[0],usecols=[0], encoding='ISO-8859-1', engine='python')\n",
    "    raw_label = pd.read_csv(filename[0],usecols=[2], encoding='ISO-8859-1', engine='python')\n",
    "    raw_label2 = pd.read_csv(filename[0],usecols=[4], encoding='ISO-8859-1', engine='python')\n",
    "    raw_target = pd.read_csv(filename[0],usecols=[1], encoding='ISO-8859-1', engine='python')\n",
    "    #Transforming data\n",
    "    label = pd.DataFrame.replace(raw_label,['FAVOR','NONE','AGAINST'], [1,2,0])\n",
    "    label2 = pd.DataFrame.replace(raw_label2,['pos','other','neg'], [1,2,0])\n",
    "    target = pd.DataFrame.replace(raw_target,['Atheism','Climate Change is a Real Concern','Feminist Movement',\\\n",
    "                                              'Hillary Clinton','Legalization of Abortion'], [4,3,2,1,0])\n",
    "    \n",
    "    concat_text = pd.concat([raw_text, target, label, label2], axis=1)\n",
    "    \n",
    "    return(concat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fnm1 = 'train.csv'\n",
    "#fnm2 = 'test_A.csv'\n",
    "#train = Load_Transform_Data(fnm1)\n",
    "#df_train, df_val = train_test_split(train, test_size=0.15, random_state=RANDOM_SEED)\n",
    "#df_test = Load_Transform_Data(fnm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = Load_Transform_Data(filename1)\n",
    "df_val = Load_Transform_Data(filename2)\n",
    "df_test = Load_Transform_Data(filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Tweet'] = df_train['Tweet'].apply(Data_Clean)\n",
    "df_val['Tweet'] = df_val['Tweet'].apply(Data_Clean)\n",
    "df_test['Tweet'] = df_test['Tweet'].apply(Data_Clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer=BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, doc, targets, stance, sentiment, length, tokenizer, max_len):\n",
    "        self.doc = doc\n",
    "        self.targets = targets\n",
    "        self.stance = stance\n",
    "        self.sentiment = sentiment\n",
    "        self.length = length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.doc)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        doc = str(self.doc[item])\n",
    "        target = self.targets[item]\n",
    "        stance = self.stance[item]\n",
    "        sentiment = self.sentiment[item]\n",
    "        length = self.length[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "                    doc,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.max_len,\n",
    "                    return_token_type_ids=False,\n",
    "                    pad_to_max_length=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "                    TOKENIZERS_PARALLELISM = True\n",
    "                    )\n",
    "\n",
    "        return {\n",
    "            'doc_text': doc,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long),\n",
    "            'stance': torch.tensor(stance, dtype=torch.long),\n",
    "            'sentiment': torch.tensor(sentiment, dtype=torch.long),\n",
    "            'length': torch.tensor(length, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len,length, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        doc=df['Tweet'].to_numpy(),\n",
    "        targets=df['Target'].to_numpy(),\n",
    "        stance = df['Stance'].to_numpy(),\n",
    "        sentiment = df['Sentiment'].to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len,\n",
    "        length = length\n",
    "      )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_len = np.array([len(xi.split()) for xi in df_train.iloc[:,0]])\n",
    "x_val_len = np.array([len(xi.split()) for xi in df_val.iloc[:,0]])\n",
    "x_test_len = np.array([len(xi.split()) for xi in df_test.iloc[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 50\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, x_train_len, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, x_val_len, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, x_test_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_tar_size = 150\n",
    "linear_stc_size = 300\n",
    "linear_sent_size = 250\n",
    "lambd = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_Sentiment(hidden_unit, last_unit, W_h, W_z, b_tanh, v, length):\n",
    "    \n",
    "    s1 = hidden_unit.size(0) # batch size 16\n",
    "    s2 = hidden_unit.size(1) #sequence length 50\n",
    "    s3 = hidden_unit.size(2) # hidden dimension 768\n",
    "    \n",
    "    # 16x50x768 x 768x768 + 16x768 x 768x768 +1x768 = 16x50x768 + 16x1x768 + 1x768 = 16x50x768\n",
    "    m1 = torch.mm(hidden_unit.contiguous().view(-1,s3),W_h).view(-1, s2, s3)\n",
    "    # 16x768 x 768x768 = 16x1x768\n",
    "    m2 = torch.mm(last_unit.contiguous().view(-1,s3),W_z).view(-1, 1, s3)\n",
    "#   16x768 x 768x768 = 16x768\n",
    "\n",
    "    # 16x50x768 + 16x1x768 + 1x768 = 16x50x768\n",
    "    sum_tanh = nn.functional.tanh(m1 + m2 + b_tanh.unsqueeze(0))\n",
    "    \n",
    "    #16x50x768 x 768x1 = 16x50x1->16x50\n",
    "    u = torch.mm(sum_tanh.contiguous().view(-1,s3),v.unsqueeze(1)).view(-1,s2,1).squeeze(2)\n",
    "\n",
    "    for i in range(len(length)):\n",
    "        u[i, length[i]:] = torch.Tensor([-1e6])\n",
    "    \n",
    "    # alphas size 16x50\n",
    "    alphas = nn.functional.softmax(u)        \n",
    "\n",
    "   \n",
    "    #16x1x50 x 16x50x768 = 16x768\n",
    "    context = torch.bmm(alphas.unsqueeze(1), hidden_unit).squeeze(1)\n",
    "\n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Attention_Stance(hidden_unit,last_unit, W_h, W_z, b_tanh, v, length):\n",
    "    \n",
    "\n",
    "    s1 = hidden_unit.size(0) # batch size 16\n",
    "    s2 = hidden_unit.size(1) #sequence length 50\n",
    "    s3 = hidden_unit.size(2) # hidden dimension 768\n",
    "    \n",
    "#    word_tensor = torch.zeros(s1,s2,300).to(device) #16x?x300\n",
    "#    word_tensor[:,:,:] = target_word\n",
    "\n",
    "    \n",
    "\n",
    "    # 16x50x768 x 768x768 = 16x50x768\n",
    "    m1 = torch.mm(hidden_unit.contiguous().view(-1,s3),W_h).view(-1, s2, s3)\n",
    "    #16x768 x 768x768 = 16x768 = 16x1x768\n",
    "    m2 = torch.mm(last_unit.contiguous().view(-1,s3),W_z).view(-1, 1, s3)\n",
    "    #16x50x768 + 16x1x768 + 1x768 = 16x50x768\n",
    "    sum_tanh = nn.functional.tanh(m1 + m2 + b_tanh.unsqueeze(0))\n",
    "\n",
    "    #16x50x768 x 768x1 = 16x50\n",
    "    u = torch.mm(sum_tanh.contiguous().view(-1,s3),v.unsqueeze(1)).view(-1,s2,1).squeeze(2)\n",
    "    \n",
    "    for i in range(len(length)):\n",
    "        u[i, length[i]:] = torch.Tensor([-1e6])\n",
    "\n",
    "  \n",
    "    # alphas size 16x50\n",
    "    alphas = nn.functional.softmax(u)        \n",
    "\n",
    "    # context size 16x1x50 x 16x50x768 = 16x1x768 = 16x768\n",
    "    context = torch.bmm(alphas.unsqueeze(1), hidden_unit).squeeze(1)\n",
    "\n",
    "    return context, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.bert_stc = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        \n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "\n",
    "        self.linear_stc = nn.Linear(self.bert.config.hidden_size*2, linear_stc_size)\n",
    "        self.linear_sent = nn.Linear(self.bert.config.hidden_size, linear_sent_size)\n",
    "        \n",
    "\n",
    "        self.out_stc = nn.Linear(linear_stc_size, 3)\n",
    "        self.out_sent = nn.Linear(linear_sent_size, 3)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.rand([self.bert.config.hidden_size,self.bert.config.hidden_size],requires_grad=True))\n",
    "        self.W_z = nn.Parameter(torch.rand([self.bert.config.hidden_size,self.bert.config.hidden_size],requires_grad=True))\n",
    "        self.b_tanh = nn.Parameter(torch.rand(self.bert.config.hidden_size,requires_grad=True))\n",
    "        self.v = nn.Parameter(torch.rand(self.bert.config.hidden_size,requires_grad=True))\n",
    "        \n",
    "        self.W_h2 = nn.Parameter(torch.rand([self.bert.config.hidden_size,self.bert.config.hidden_size],requires_grad=True))\n",
    "        self.W_z2 = nn.Parameter(torch.rand([768,self.bert.config.hidden_size],requires_grad=True))\n",
    "        self.b_tanh2 = nn.Parameter(torch.rand(self.bert.config.hidden_size,requires_grad=True))\n",
    "        self.v2 = nn.Parameter(torch.rand(self.bert.config.hidden_size,requires_grad=True))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, x_len):\n",
    "        output, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        output1, pooled_output1 = self.bert_stc(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        drop1 = self.drop(pooled_output)\n",
    "        drop2 = self.drop(pooled_output1)\n",
    "        drop3 = self.drop(output)\n",
    "        drop4 = self.drop(output1)\n",
    "        \n",
    "        atten, alphas = Attention_Sentiment(drop3, drop1,self.W_h,self.W_z,self.b_tanh,self.v,x_len)\n",
    "        atten2, alphas_main = Attention_Stance(drop4, drop2, self.W_h2,self.W_z2,self.b_tanh2,self.v2,x_len)\n",
    "        \n",
    "        linear = self.relu(self.linear_sent(atten)) # 16x768->16x250\n",
    "        out_sent = self.out_sent(self.drop(linear))#16x250->16x3\n",
    "        \n",
    "        combine = torch.cat((atten,atten2),1) # 16x768+16x768 -> 16x1536\n",
    "\n",
    "\n",
    "        lin2 = self.relu(self.linear_stc(combine))#16x1536->16x550\n",
    "        \n",
    "        out_stc = self.out_stc(self.drop(lin2))#16x5sq50->16x3\n",
    "\n",
    "        \n",
    "        \n",
    "        return out_sent, out_stc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "    ):\n",
    "    model = model.train()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions_sent = 0\n",
    "    correct_predictions_stc = 0\n",
    "\n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        stance = d['stance'].to(device)\n",
    "        sentiment = d['sentiment'].to(device)\n",
    "        x_len = d['length'].to(device)\n",
    "\n",
    "        out_sent, out_stc = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask,\n",
    "            x_len = x_len\n",
    "        )\n",
    "\n",
    "        _, pred_sent = torch.max(out_sent, dim=1)\n",
    "        _, pred_stc = torch.max(out_stc, dim=1)\n",
    "        \n",
    "\n",
    "        loss = (1-lambd)*loss_fn(out_sent, sentiment) + lambd*loss_fn(out_stc, stance)\n",
    "        \n",
    "\n",
    "        correct_predictions_sent += torch.sum(pred_sent == sentiment)\n",
    "        correct_predictions_stc += torch.sum(pred_stc==stance)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return correct_predictions_sent.double() / n_examples, correct_predictions_stc.double() / n_examples,np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions_sent = 0\n",
    "    correct_predictions_stc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            stance = d['stance'].to(device)\n",
    "            sentiment = d['sentiment'].to(device)\n",
    "            x_len = d['length'].to(device)\n",
    "\n",
    "            out_sent, out_stc = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            x_len = x_len\n",
    "            )\n",
    "            _, pred_sent = torch.max(out_sent, dim=1)\n",
    "            _, pred_stc = torch.max(out_stc, dim=1)\n",
    "\n",
    "            loss = (1-lambd)*loss_fn(out_sent, sentiment) + lambd*loss_fn(out_stc, stance)\n",
    "        \n",
    "\n",
    "            correct_predictions_sent += torch.sum(pred_sent == sentiment)\n",
    "            correct_predictions_stc += torch.sum(pred_stc == stance)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions_sent.double() / n_examples, correct_predictions_stc.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "\n",
    "    review_texts = []\n",
    "    predictions_sent = []\n",
    "    prediction_probs_sent = []\n",
    "    real_values_sent = []\n",
    "    \n",
    "    predictions_stc = []\n",
    "    prediction_probs_stc = []\n",
    "    real_values_stc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "\n",
    "            texts = d[\"doc_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            stance = d['stance'].to(device)\n",
    "            sentiment = d['sentiment'].to(device)\n",
    "            x_len = d['length'].to(device)\n",
    "\n",
    "            out_sent, out_stc = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            x_len = x_len\n",
    "            )\n",
    "            _, pred_sent = torch.max(out_sent, dim=1)\n",
    "            _, pred_stc = torch.max(out_stc, dim=1)\n",
    "\n",
    "            probs_sent = F.softmax(out_sent, dim=1)\n",
    "            probs_stc = F.softmax(out_stc, dim=1)\n",
    "\n",
    "            review_texts.extend(texts)\n",
    "            predictions_sent.extend(pred_sent)\n",
    "            prediction_probs_sent.extend(probs_sent)\n",
    "            real_values_sent.extend(sentiment)\n",
    "            \n",
    "            predictions_stc.extend(pred_stc)\n",
    "            prediction_probs_stc.extend(probs_stc)\n",
    "            real_values_stc.extend(stance)\n",
    "\n",
    "    predictions_sent = torch.stack(predictions_sent).to(device)\n",
    "    prediction_probs_sent = torch.stack(prediction_probs_sent).to(device)\n",
    "    real_values_sent = torch.stack(real_values_sent).to(device)\n",
    "    \n",
    "    predictions_stc = torch.stack(predictions_stc).to(device)\n",
    "    prediction_probs_stc = torch.stack(prediction_probs_stc).to(device)\n",
    "    real_values_stc = torch.stack(real_values_stc).to(device)\n",
    "    \n",
    "    return review_texts, predictions_sent, prediction_probs_sent, real_values_sent, predictions_stc, prediction_probs_stc, real_values_stc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-d1ef291ceac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-234-9eb60ff31ec3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "class_names = ['abortion', 'hillary', 'feminism', 'climate', 'atheism']\n",
    "class_names1 = ['AGAINST','FAVOR', 'NONE']\n",
    "class_names2 = ['neg', 'pos','other']\n",
    "\n",
    "model = SentimentClassifier().to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay = 1e-2, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 20)\n",
    "\n",
    "    train_acc_sent, train_acc_stc, train_loss = train_epoch(model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "        )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy_sent {train_acc_sent} accuracy_stc {train_acc_stc}')\n",
    "#    print(f'Train loss {train_loss} accuracy_sent {train_acc_sent}')\n",
    "\n",
    "    val_acc_sent,val_acc_stc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "        )\n",
    "\n",
    "    print(f'Val loss {val_loss} accuracy_sent {val_acc_sent} accuracy_stc {val_acc_stc}')\n",
    "#    print(f'Val loss {val_loss} accuracy_sent {val_acc_sent}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc_sent'].append(train_acc_sent)\n",
    "    history['train_acc_stc'].append(train_acc_stc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc_sent'].append(val_acc_sent)\n",
    "    history['val_acc_stc'].append(val_acc_stc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc_stc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc_stc\n",
    "        \n",
    "    test_acc_sent,test_acc_stc, _ = eval_model(\n",
    "          model,\n",
    "          test_data_loader,\n",
    "          loss_fn,\n",
    "          device,\n",
    "          len(df_test)\n",
    "        )\n",
    "\n",
    "print('\\nTest Accuracy_sent:\\n')\n",
    "print(test_acc_sent.item())\n",
    "print('\\nTest Accuracy_stc:\\n')\n",
    "print(test_acc_stc.item())\n",
    "\n",
    "y_review_texts, y_pred_sent, y_pred_probs_sent, y_test_sent, y_pred_stc, y_pred_probs_stc, y_test_stc= get_predictions(model,test_data_loader)\n",
    "\n",
    "print('\\n Sentiment prediction:\\n')\n",
    "print(classification_report(y_test_sent, y_pred_sent, target_names=class_names2))\n",
    "print('Stance prediction:\\n')\n",
    "print(classification_report(y_test_stc, y_pred_stc, target_names=class_names1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
